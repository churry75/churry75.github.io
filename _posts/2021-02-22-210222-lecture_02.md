---
title: "2021.02.22 lecture(2)"
excerpt: "딥러닝 기초"
toc: true
toc_sticky: true
published: false

categories:
    - 자율주행스쿨강의
tags:
    - Lecture
    - Deep Learning
    - DQN

last_modified_at: 2021.02.22-11:43:05  
---

>이 페이지는 프로그래머스 ((주)그렙)에서 진행하는\
**[K-Digital-Training] 자율주행 데브코스**에서 배운 내용 및 강의자료를 인용하여 작성한 것입니다.

# MNIST를 이용한 Classification 코드
## 실습 환경 구성
- 페이스북에서 개발한 파이썬을 위한 딥러닝 라이브러리인 **PyTorch** 사용
    > 구현이 직관적이고 간결하여 처음 딥러닝을 시작하는 사용자들에게 좋음

- [공식 홈페이지](https://pytorch.org/)를 참고하여 설치 진행

## MNIST란?
- 0 ~ 9로 이루어진 손글씨 숫자 데이터
- 28 * 28 크기의 흑백 이미지
- 60,000개의 학습데이터, 10,000개의 테스트 데이터로 구성

## MNIST 실습
### 개요
- 목표: PyTorch를 이용해 MNIST 데이터를 분류하는 CNN 네트워크 구현
- 네트워크 구조
    - 3번의 convolution, 3번의 인공신경망 연산 수행
    - 출력은 총 10개의 값을 도출
        - 각 값은 입력이 0 ~ 9 중 각 숫자일 확률을 나타냄
        - ex) 출력: [0, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.1, 0.2]
            - 입력이 0일 확률 = 0%
            - 입력이 1 ~ 8일 확률 = 10%
            - 입력이 9일 확률 = 20%

### 코드 작성
1. 라이브러리 불러오기
    ```python
    import torch
    import torch.nn as nn
    import torch.nn.functional as F
    import torch.optim as optim
    from torchvision import datasets, transforms

    import numpy as np

    device = torch.device("cuda:0" if torch.cuda.is_available() else "cpu")
    ```

    - PyTorch 관련 라이브러리 호출
    - gpu가 있는 경우에 연산을 gpu로, 아니면 cpu로 수행하도록 명령어 추가

2. 하이퍼파라미터 설정
    ```python
    batch_size = 128
    num_epochs = 10

    learning_rate = 0.00025
    ```

    - 배치의 크기(한 번 학습시 사용할 데이터의 크기), 학습 수행 횟수, 학습률 설정
    - 학습률이 너무 작으면 학습 속도가 느리고, 값이 크면 최적으로 학습하지 못 함

3. MNIST 데이터 다운로드
    ```python
    # training dateset
    trn_dataset = datasets.MNIST('./mnist_data/', download=True,
                                 train=True,
                                 transform=transforms.Compose([
                                     transforms.ToTensor()]))
    # validation dataset
    val_dataset = datasets.MNIST('./mnist_data/', download=False,
                                 train=False,
                                 transform=transforms.Compose([
                                     transforms.ToTensor()]))
    ```

    - `train=True`: True인 경우 학습 데이터로 사용, False인 경우 학습에 사용하지 않는 데이터
    - `transform=transforms.Compose([transforms.ToTensor()])`: Tensor의 형태로 데이터 출력
        > PyTorch 연산의 경우 tensor의 형태로 연산 진행

4. DataLoader 설정
    ```python
    trn_loader = torch.utils.data.DataLoader(trn_dataset,
                                             batch_size=batch_size,
                                             shuffle=True)

    val_loader = torch.utils.data.DataLoader(val_dataset,
                                             batch_size=batch_size,
                                             shuffle=True)
    ```

    - 학습 및 검증 데이터를 섞어준 후 미니배치로 나눠주는 PyTorch의 기본 기능
    
5. CNN 네트워크 구성
    ```python
    class CNNClassifier(nn.Module):
        def __init__(self):
            super(CNNClassifier, self).__init__() # 항상 torch.nn.Module을 상속 받고 시작
            # 네트워크 연산에 사용할 구조 설정
            self.conv1 = nn.Conv2d(1, 16, 3, 2)
            self.conv2 = nn.Conv2d(16, 32, 3, 2)
            self.conv3 = nn.Conv2d(32, 64, 3, 1)

            self.fc1 = nn.Linear(64*4*4, 256)
            self.fc2 = nn.Linear(256, 64)
            self.fc3 = nn.Linear(64, 10)

        # 네트워크 연산 수행
        def forward(self, x):
            x = F.relu(self.conv1(x))
            x = F.relu(self.conv2(x))
            x = F.relu(self.conv3(x))
            x = x.view(x.size(0), -1)

            x = F.relu(self.fc1(x))
            x = F.relu(self.fc2(x))
            x = self.fc3(x)
            return F.softmax(x, dim=1)
    ```

    - `nn.Conv2d(입력의 채널 수, 출력의 채널 수, Kernal size, Stride)`: Convolution 연산을 위한 함수
    - `nn.Linear(입력 노드의 수, 출력 노드의 )`: 인공 신경망 연산을 위한 함수
        > 마지막 연산에서 출력 도출
    
    - `x = x.view(x.size(0), -1)`: 4차원의 Feature Map 데이터를 2차원의 Vector 데이터로 변환
        > 인경 신경망 연산을 위해 데이터를 벡터로 변환하는 과정이 필요함
    
    - 최종 결과에 Softmax 연산 수
        > Softmax: 출력 결과를 확률로 변환(요소의 총합이 1이 되도록)\
        입력에 대한 결과의 확률을 알 수 있음

6. 정확도 도출 함수 정의
    ```python
    def get_accuracy(y, label):
        y_idx = torch.argmax(y, dim=1)
        result = y_idx - label

        num_correct = 0
        for i in range(len(result)):
            if result[i] == 0:
                num_correct += 1

        return num_correct/y.shape[0]
    ```

    - `y`: 네트워크의 연산 결과, `label`: 정답
    - `torch.argmax()`: 확률 중 가장 큰 값의 인덱스를 반환
    - 모든 입력에 대해 정답을 맞춘 개수를 전체 개수로 나눠주어 정확도를 반환

7. 네트워크, 손실함수, 최적화 선언
    ```python
    cnn = CNNClassifier().to(device)  # 설정한 device로 딥러닝 네트워크 연산을 하도록 설정
    criterion = nn.CrossEntropyLoss()  # 손실함수 설정
    optimizer = optim.Adam(cnn.parameters(), lr=learning_rate)  # 최적화 설정

    num_batches = len(trn_loader)  # 한 epoch에 대한 전체 미니배치의 수
    ```

    - Adam Optimizer: 딥러닝에서 주로 사용하는 최적화 함수,\
    cnn 네트워크의 파라미터, 학습률 설정

8. 학습 및 검증 수행
    ```python
    for epoch in range(num_epochs):
        trn_loss_list = []  # 손실함수 값 저장
        trn_acc_list = []  # 정확도 저장
        # 1 epoch 연산을 위한 반복문
        for i, data in enumerate(trn_loader):
            # 데이터 처리, 네트워크 학습을 위한 모드로 설정
            cnn.train()
            # 학습 데이터(x: 입력, label: 정답)를 받아온 후 device에 올려줌
            x, label = data
            x = x.to(device)
            label = label.to(device)

            # 네트워크 연산 및 손실함수 계산
            model_output = cnn(x)
            loss = criterion(model_output, label)  # 손실함수 계산

            # 네트워크 업데이트
            optimizer.zero_grad()  # 학습 수행 전 미분값을 0으로 초기화(학습 전 반드시 처리 필요)
            loss.backward()  # 가중치와 편향에 대한 기울기 계산
            optimizer.step()  # 가중치와 편향 업데이트

            # 학습 정확도 및 손실함수 값 기록
            trn_acc = get_accuracy(model_output, label)  # 정확도 계산

            trn_loss_list.append(loss.item())
            trn_acc_list.append(trn_acc)

            # 학습 진행 상황 출력 및 검증셋 연산 수행
            if (i+1) % 100 == 0:  # 매 100번째 미니배치 연산마다 진행상황 출력
                cnn.eval()  # 네트워크를 검증 모드로 설정
                with torch.no_grad():  # 학습에 사용하지 않는 코드들은 해당 블록 내에 기입
                    val_loss_list = []  # 검증 시 손실함수 값 저장
                    val_acc_list = []  # 검증 시 정확도 저

                    # 검증 셋에 대한 연산 수행
                    for j, val in enumerate(val_loader):
                        val_x, val_label = val

                        val_x = val_x.to(device)
                        val_label = val_label.to(device)

                        val_output = cnn(val_x)

                        val_loss = criterion(val_output, val_label)
                        val_acc = get_accuracy(val_output, val_label)

                        val_loss_list.append(val_loss.item())
                        val_acc_list.append(val_acc)

                print("epoch: {}/{} | step: {}/{} | trn loss: {:.4f} | val loss: {:.4f} | trn_acc: {:.4f} | val_acc: {:.4f}".format(epoch+1, num_epochs, i+1, num_batches, np.mean(trn_loss_list), np.mean(val_loss_list), np.mean(trn_acc_list), np.mean(val_acc_list)))
    ```

    - `trn_loss_list.append(loss.item())`: `.item()`은 하나의 값으로 된 tensor를 일반 값으로 바꿔줌

### 결과
- **학습 정확도: 89.25 % | 검증 정확도: 89.16 %**
- overfitting은 발생하지 않은 것으로 보임\
![result](/assets/images/lecture/week13_imgs/MNIST_result.png)
