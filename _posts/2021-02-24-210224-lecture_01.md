---
title: "2021.02.24 lecture(1)"
excerpt: "Q-Learning 한계와 DQN"
toc: true
toc_sticky: true
published: true

categories:
    - 자율주행스쿨강의
tags:
    - Lecture
    - Deep Learning
    - DQN

last_modified_at: 2021.02.24-11:54:23
---

>이 페이지는 프로그래머스 ((주)그렙)에서 진행하는\
**[K-Digital-Training] 자율주행 데브코스**에서 배운 내용 및 강의자료를 인용하여 작성한 것입니다.


# Q-Learning의 한계
- Q-Learning은 Q-table 안에 있는 여러 state와 action에 대한 Q-value를 학습하는 것이 목표이다.
- 스타크래프트와 같은 전략시뮬레이션 게임 혹은 방대한 선택을 해야만 하는 경우에는 경우의 수가 너무 많아지기 때문에 이를 포괄할 수 있는 Q-Table을 만드는 것은 거의 불가능하다.
    - **즉, state가 너무 다양하면 하나의 input에 대해서 하나의 output을 내놓는 Q-table을 만들기가 어렵다.**
    - 따라서, 학습을 함에 있어서 table을 대신하여 결과를 예측 또는 도출을 할 수 있는 **함수**를 사용할 필요가 있다.
        - 이러한 한계점을 개선하는 것이 Neural Network, DQN의 concept 이다.

# DQN(Deep Q Network)
![image](/assets/images/lecture/week13_imgs/210224_01.jpeg)

# CartPole 실습 결과
![result](/assets/images/lecture/week13_imgs/dqn_result.png)
- 훈련이 진행되어도 딱히 학습이 되지 않음
- DQN의 성능을 안정적으로 만들어주는 추가적인 기법이 적용이 안되서 그렇다고 함
